{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "num_channels = 3\n",
    "hidden_size = 768\n",
    "patch_size = 16\n",
    "conv = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 140, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "fake_image = torch.rand((1, 3, 224, 168))\n",
    "\n",
    "print(conv(fake_image).reshape(1, -1, 768).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.vit.modeling_vit import ViTPatchEmbeddings\n",
    "\n",
    "embs = ViTPatchEmbeddings(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input image size (224*168) doesn't match model (224*224).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p3/r4w7hxbd6bb31zz17fz30vgw0000gn/T/ipykernel_30397/3314632781.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/forms/ml_framework/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/forms/ml_framework/venv/lib/python3.9/site-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    178\u001b[0m                     \u001b[0;34mf\"Input image size ({height}*{width}) doesn't match model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                     \u001b[0;34mf\" ({self.image_size[0]}*{self.image_size[1]}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input image size (224*168) doesn't match model (224*224)."
     ]
    }
   ],
   "source": [
    "embs(fake_image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to compute the positional embeddings\n",
    "\n",
    "# BS, hidden dim, x patches, y patches\n",
    "patches = conv(fake_image)\n",
    "# print(patches.shape)\n",
    "\n",
    "# To project to the right hidden embedding dim\n",
    "hidden_dim = 768\n",
    "x_projection = torch.nn.Linear(1, hidden_dim)\n",
    "y_projection = torch.nn.Linear(1, hidden_dim)\n",
    "\n",
    "batch_size, x_size, y_size = patches.shape[0], patches.shape[2], patches.shape[3]\n",
    "\n",
    "# x_embeddings\n",
    "patches_x_embeddings = torch.arange(x_size).view(batch_size, 1, -1) / x_size\n",
    "patches_x_embeddings = x_projection(patches_x_embeddings.T).view(batch_size, x_size, hidden_dim)\n",
    "patches_x_embeddings = patches_x_embeddings.expand(y_size, batch_size, x_size, hidden_dim)\n",
    "patches_x_embeddings = patches_x_embeddings.reshape(batch_size, -1, hidden_dim)\n",
    "\n",
    "# y_embeddings\n",
    "patches_y_embeddings = torch.arange(y_size).view(batch_size, 1, -1) / y_size\n",
    "patches_y_embeddings = y_projection(patches_y_embeddings.T).view(batch_size, y_size, hidden_dim)\n",
    "patches_y_embeddings = patches_y_embeddings.expand(x_size, batch_size, y_size, hidden_dim)\n",
    "patches_y_embeddings = patches_y_embeddings.reshape(batch_size, -1, hidden_dim)\n",
    "\n",
    "patches_positional_embeddings = patches_x_embeddings + patches_y_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches=2048\n",
      "num_patches=272\n"
     ]
    }
   ],
   "source": [
    "# Retaining the aspect ratio for a given effective resolution\n",
    "\n",
    "img1 = torch.rand((1, 3, 1024, 512))\n",
    "img2 = torch.rand((1, 3, 256, 272))\n",
    "\n",
    "patches_1 = conv(img1)\n",
    "patches_2 = conv(img2)\n",
    "print(f'num_patches={patches_1.shape[-1]*patches_1.shape[-2]}')\n",
    "print(f'num_patches={patches_2.shape[-1]*patches_2.shape[-2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024 / 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 2048])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = torch.rand((1, 3, 1024, 512))\n",
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.4) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p3/r4w7hxbd6bb31zz17fz30vgw0000gn/T/ipykernel_88827/617130864.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mrounded_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# resized_img = im.resize(())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounded_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounded_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounded_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounded_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounded_x\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrounded_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.4) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
     ]
    }
   ],
   "source": [
    "# Retaining the aspect ratio for a given effective resolution\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import cv2 \n",
    "\n",
    "num_channels = 3\n",
    "hidden_size = 768\n",
    "patch_size = 16\n",
    "conv = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "effective_resolution = 512\n",
    "\n",
    "for x_size, y_size in [(1024, 512), (1024, 1024), (256, 256), (1121, 456)]:\n",
    "    imarray = torch.rand(x_size, y_size,3) * 255\n",
    "    # im = Image.fromarray(imarray.astype('uint8'))\n",
    "\n",
    "    aspect_ratio = x_size / y_size\n",
    "\n",
    "    new_y = np.sqrt(effective_resolution**2/aspect_ratio)\n",
    "    new_x = new_y * aspect_ratio\n",
    "    \n",
    "    \n",
    "    rounded_y = math.floor(new_y)\n",
    "    rounded_x = math.floor(new_x)\n",
    "    # resized_img = im.resize(())\n",
    "    res = cv2.resize(imarray, dsize=(rounded_y, rounded_x), interpolation=cv2.INTER_CUBIC)\n",
    "    print(rounded_y, rounded_x)\n",
    "    print(rounded_x / rounded_y, aspect_ratio)\n",
    "    print(effective_resolution**2, rounded_x*rounded_y)\n",
    "    img = torch.tensor(res, dtype=torch.float).view(3, rounded_x, rounded_y).unsqueeze(0)\n",
    "\n",
    "    patches = conv(img)\n",
    "    print(f'num_patches={patches.shape[-1]*patches.shape[-2]}')\n",
    "    \n",
    "    # Then we need to pad to the max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 1024])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = patches.view(1, hidden_size, -1)\n",
    "seq_length = patches.shape[-1]\n",
    "max_patches = int((effective_resolution**2)/(patch_size**2))\n",
    "torch.nn.functional.pad(patches, (0,max_patches-seq_length)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 83.43414  ,   4.775781 , 217.92708  ],\n",
       "        [ 66.16865  ,  37.061966 ,  31.308632 ],\n",
       "        [ 25.140652 , 208.80331  , 107.35911  ],\n",
       "        ...,\n",
       "        [161.07523  , 149.33807  ,  30.532988 ],\n",
       "        [150.54028  , 213.85788  ,  67.94908  ],\n",
       "        [240.86948  ,   8.646132 ,  15.622634 ]],\n",
       "\n",
       "       [[169.03717  , 221.36116  , 165.24219  ],\n",
       "        [147.3549   , 162.00345  ,  56.119705 ],\n",
       "        [242.51111  , 123.3208   , 119.07459  ],\n",
       "        ...,\n",
       "        [109.28696  ,  72.45307  ,  21.083914 ],\n",
       "        [183.39742  ,  15.4071245, 145.85396  ],\n",
       "        [226.925    ,  51.414204 , 104.07337  ]],\n",
       "\n",
       "       [[ 41.385983 , 230.80757  , 224.42665  ],\n",
       "        [162.5485   , 186.38916  ,  86.466805 ],\n",
       "        [ 34.31891  , 153.07722  ,  56.078804 ],\n",
       "        ...,\n",
       "        [185.42879  , 202.80441  , 192.44173  ],\n",
       "        [ 90.75602  , 245.34912  , 226.29155  ],\n",
       "        [139.18805  ,  78.44048  ,  29.52753  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[248.85818  , 125.35989  ,  37.49262  ],\n",
       "        [250.96574  ,  94.53272  ,  36.849495 ],\n",
       "        [ 74.229355 ,  90.18579  , 177.37453  ],\n",
       "        ...,\n",
       "        [ 92.87202  , 110.02089  ,  98.818275 ],\n",
       "        [ 43.69279  , 205.92691  ,  58.454224 ],\n",
       "        [132.02608  , 100.22026  , 225.96541  ]],\n",
       "\n",
       "       [[183.00172  , 120.40473  , 154.39882  ],\n",
       "        [238.09258  , 212.58403  ,  43.841076 ],\n",
       "        [204.49655  , 180.68245  , 121.98788  ],\n",
       "        ...,\n",
       "        [245.40974  , 196.79337  , 126.2867   ],\n",
       "        [222.86752  , 155.55484  , 147.80338  ],\n",
       "        [ 70.00536  , 100.7161   , 219.96738  ]],\n",
       "\n",
       "       [[242.12532  , 119.21723  ,  94.73752  ],\n",
       "        [219.69734  ,   8.774185 ,   3.16026  ],\n",
       "        [146.26482  ,  23.44994  ,  46.36607  ],\n",
       "        ...,\n",
       "        [ 89.0698   ,  66.12905  ,  35.245388 ],\n",
       "        [243.94737  ,  46.7293   , 219.92798  ],\n",
       "        [123.82954  , 242.66382  ,  58.450546 ]]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imarray.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.25"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "724/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720 720\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "image_height, image_width = 1024, 1024\n",
    "\n",
    "# This will set the target resolution for resizing\n",
    "max_patches = 2048\n",
    "patch_height=patch_width=16\n",
    "\n",
    "scale = math.sqrt(max_patches * (patch_height / image_height) * (patch_width / image_width))\n",
    "num_feasible_rows = max(min(math.floor(scale * image_height / patch_height), max_patches), 1)\n",
    "num_feasible_cols = max(min(math.floor(scale * image_width / patch_width), max_patches), 1)\n",
    "resized_height = max(num_feasible_rows * patch_height, 1)\n",
    "resized_width = max(num_feasible_cols * patch_width, 1)\n",
    "\n",
    "print(resized_height, resized_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524288"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524288"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024*512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBartTokenizer'. \n",
      "The class this function is called from is 'XLMRobertaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\n",
    "            \"hyunwoongko/asian-bart-ecjk\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> A<unk></s>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer('A1')['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
